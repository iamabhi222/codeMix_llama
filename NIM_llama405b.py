import json
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
from openai import OpenAI

NVIDAI_API_KEY = "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"

client = OpenAI(
    base_url="https://integrate.api.nvidia.com/v1",
    api_key=NVIDAI_API_KEY
)

# Function to count the number of tokens in a sentence
def count_tokens(sentence):
    if isinstance(sentence, str):
        return len(sentence.split())
    return 0

# Function to create batches of sentences with a maximum token limit
def create_batches(sentences, max_tokens=512):
    batches = []
    current_batch = []
    current_tokens = 0

    for item in sentences:
        src_sent = item.get('src_sent', '')
        tgt_sent = item.get('tgt_sent', '')
        src_tokens = count_tokens(src_sent)
        tgt_tokens = count_tokens(tgt_sent)
        total_tokens = src_tokens + tgt_tokens

        if current_tokens + total_tokens > max_tokens:
            batches.append(current_batch)
            current_batch = [item]
            current_tokens = total_tokens
        else:
            current_batch.append(item)
            current_tokens += total_tokens

    if current_batch:
        batches.append(current_batch)

    return batches

# Load sentences from JSON file
with open('sentences.json', 'r', encoding='utf-8') as file:
    data = json.load(file)

# Define your desired data structure.
class StructOBJ(BaseModel):
    src_sent: str = Field(description="The provided source sentences.")
    tgt_sent: str = Field(description="The provided target sentences.")
    codeMix: str = Field(description="The target codeMixed sentences generated by the model.")

# Initialize dictionary to hold responses for each category
responses = {}
cnt = 0

# Create batches
batches = create_batches(data)

print(f"# Number of batches: {len(batches)}\n")

for batch in batches:
    cnt += 1
    print(f"Batch {cnt}\n")

    # Set up a parser + inject instructions into the prompt template.
    parser = JsonOutputParser(pydantic_object=StructOBJ)

    # Create prompt with the batch of sentences
    prompt = (
        "You will be provided with a list of source sentences and their corresponding translations. Your task is to generate improved translations for these source sentences.\n"
        "While translating, refer to the provided translations and identify the words that are enclosed in brackets. Use some of these words to create a Code-Mixed style translation.\n"
        "You also need to incorporate Code-Mixing into the translations that you generate by using the some of the words identified from the provided translations.\n\n"
        f"Here is the list of sentences along with their corresponding translations:\n{json.dumps(batch, ensure_ascii=False)}\n"
        f"{parser.get_format_instructions()}"
    )

    print(prompt)

    try:
        # Make the API call
        completion = client.chat.completions.create(
            model="meta/llama-3.1-405b-instruct",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            top_p=0.7,
            max_tokens=4096,
            stream=False
        )

        # Initialize buffer to store chunks of the response
        try:
            response_buffer = completion.choices[0].message.content
            # Parse the response buffer using the parser
            try:
                parsed_response = parser.parse(response_buffer)
                if isinstance(parsed_response, list):
                    responses[f"batch_{cnt}"] = parsed_response
            except Exception as e:
                print(f"Error parsing response: {e}")
        
        except Exception as e:
            print(f"Error parsing response: {e}")

    except Exception as e:
        print(f"Error making API call: {e}")

# Save all responses to a single JSON file
with open('filtered_sentences.json', 'w', encoding='utf-8') as resp_file:
    json.dump(responses, resp_file, ensure_ascii=False, indent=2)

print("Processing complete. Responses saved to filtered_sentences.json.")
